{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatoJin02/BDA_course25/blob/main/s1290074_11_task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: IRIS Dataset - Data Exploration and Naive Bayes Classification**"
      ],
      "metadata": {
        "id": "yp4vhtWyPAla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Data Exploration Summary\n",
        "\n",
        "The IRIS dataset (150 total samples, 4 features: sepal/petal length/width) was loaded. Data exploration confirmed that petal length and petal width are the most effective features for separating the classes. The Setosa class is linearly separable, but Versicolor and Virginica show a slight overlap in their feature space."
      ],
      "metadata": {
        "id": "aSlWn4QoP_oE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Naive Bayes Model Implementation (Code)\n",
        "\n",
        "The Gaussian Naive Bayes model was implemented using a 70% training / 30% testing split"
      ],
      "metadata": {
        "id": "UN5PtLMOQS37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the IRIS dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Create DataFrame for exploration (as shown in console output)\n",
        "df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "df['species_name'] = y\n",
        "df['species_name'] = df['species_name'].apply(lambda x: target_names[x])\n",
        "\n",
        "print(\"--- Data Exploration Summary ---\")\n",
        "print(\"Features:\", iris.feature_names)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "# sns.pairplot(df, hue='species_name', diag_kind='kde') # Plotting code removed from markdown for brevity\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Instantiate and train the Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Exploration Summary ---\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "\n",
            "First 5 rows:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "  species_name  \n",
            "0       setosa  \n",
            "1       setosa  \n",
            "2       setosa  \n",
            "3       setosa  \n",
            "4       setosa  \n",
            "\n",
            "--- Evaluation Results ---\n",
            "Accuracy Score: 0.9778\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       1.00      0.92      0.96        13\n",
            "   virginica       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[19  0  0]\n",
            " [ 0 12  1]\n",
            " [ 0  0 13]]\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2WesMPUJOPrg",
        "outputId": "06adada5-67ac-44c6-8de5-a933c11c83dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Results Summary and Observation\n",
        "\n",
        "The Gaussian Naive Bayes model achieved a high Accuracy of 0.9778.\n",
        "The Setosa class was perfectly classified (Precision/Recall/F1 = 1.00). The Confusion Matrix showed only one Versicolor sample misclassified as Virginica. The model's overall strong performance confirms that the independence assumption of Naive Bayes works well for this dataset."
      ],
      "metadata": {
        "id": "hRoQMGR3Q-2h"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}